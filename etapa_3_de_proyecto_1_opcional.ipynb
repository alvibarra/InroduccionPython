{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMFNEp11KLY/sSzh/fOFEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvibarra/InroduccionPython/blob/main/etapa_3_de_proyecto_1_opcional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNHa6S4yrNcq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('¡Hola! Soy un ChatBot.')\n",
        "print('Me crearon en 2025 como parte de un ejercicio.')\n",
        "print('¿Y tu nombre cual es?')\n",
        "nombre_usuario = str(input())\n",
        "print('¡Me encanta tu nombre, ' + nombre_usuario + \"!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAU1BT9mrTGX",
        "outputId": "b04344ad-5fb0-4ac9-a3c4-667f632045fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Hola! Soy un ChatBot.\n",
            "Me crearon en 2025 como parte de un ejercicio.\n",
            "¿Y tu nombre cual es?\n",
            "Alvaro I\n",
            "¡Me encanta tu nombre, Alvaro I!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar la biblioteca gensim en el entorno virtual de Python, porque no esta incluida por default en el entorno de Colab.\n",
        "# Nota: Al momento de instalar la biblioteca por primera vez surante la sesion se requiere permitir reiniciar la sesión de Colab.\n",
        "\n",
        "%pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqDFycUbukQm",
        "outputId": "23fba802-9008-46c6-90b0-2bcb73635456"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar los componentes (bibliotecas) re, nltk, numpy, y algunos componentes de gensim y sklearn\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# descargar las bases de datos de nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# cargar las palabras de español incluidas en la lista de stopwprds\n",
        "\n",
        "spanish_sw = set(stopwords.words('spanish'))\n",
        "#print(spanish_sw)\n",
        "\n",
        "#conectar Google Drive del usuario (de alla vamos a tomar la base de datos de frases célebres)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#leer el archivo con la base de datos de frases celebres desde Google Drive\n",
        "path = '/content/drive/MyDrive/Colab Data/'\n",
        "file = 'frases_celebres_seleccion.txt'\n",
        "\n",
        "frases = ''\n",
        "with open(path + file, 'r', errors = 'ignore') as f:\n",
        "  frases = f.read()\n",
        "  #print(frases)\n",
        ""
      ],
      "metadata": {
        "id": "4QRaIEYcxzRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Uso de la biblioteca nltk para procesar la base de datos de frases célebres\n",
        "#Para pre-procesar la base de datos de frases célebres vamos a hacer lo siguiente:\n",
        "\n",
        "#Convertir todo texto a minusculas\n",
        "#Separar las frases para conformar una lista\n",
        "#Separar palabras (opcional)\n",
        "#Solo para fines de referencia vamos a presentar en conteo de frases incluidos, ejemplos de algunos frases, y el número de palabras en la base de datos de frases célebres\n",
        "\n",
        "# convertir texto plano a lista de frases\n",
        "onlysent_tokens = nltk.sent_tokenize(frases)\n",
        "# reemplazar simbolos de nueva linea ocn espacios\n",
        "onlysent_tokens = [sentence.replace('\\n', ' ') for sentence in onlysent_tokens]\n",
        "\n",
        "# generar una copia de texto con en lowercase\n",
        "frases_lc = frases.lower()\n",
        "# convertir texto a lista de palabras\n",
        "word_tokens = nltk.word_tokenize(frases_lc)\n",
        "\n",
        "print(\"\\nNúmero de frases célebres en la base de datos:\")\n",
        "print(len(onlysent_tokens))\n",
        "\n",
        "print(\"\\nEjemplo de frases celebres:\")\n",
        "print(onlysent_tokens[0:3])\n",
        "\n",
        "print(\"\\nNúmero de palabras en la base de datos de frases célebres:\")\n",
        "print(len(word_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sTKIecz5x3v",
        "outputId": "9e2ebc49-aece-4a90-f27e-cdb946f4ea3f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número de frases célebres en la base de datos:\n",
            "862\n",
            "\n",
            "Ejemplo de frases celebres:\n",
            "['\\ufeffA buen hambre, no hay pan duro.', 'A caballo regalado no le mires el diente.', 'A Dios rogando y con el mazo dando.']\n",
            "\n",
            "Número de palabras en la base de datos de frases célebres:\n",
            "10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *** preparar datos para su uso en Word2Vec ***\n",
        "# Preprocesar texto para generar el modelo Word2Vec\n",
        "\n",
        "# quitar los simbolos de puntuación\n",
        "# convertir palabras a letras minusculas\n",
        "# y separar frases en listas de palabras\n",
        "corpus_sentences = [re.sub(r'[^\\w\\s]', '', sentence.lower()).split() for sentence in onlysent_tokens]\n",
        "\n",
        "#print(type(corpus_sentences))\n",
        "#print(len(corpus_sentences))\n",
        "#print(corpus_sentences[0:3])\n",
        "\n",
        "# funcion para eliminar stopwords\n",
        "def remove_stopwords(sentence_words, sw):\n",
        "  filtered_words = [word for word in sentence_words if word not in sw]\n",
        "  return filtered_words\n",
        "\n",
        "# eliminar stopwords de la base de datos de frases célebres\n",
        "filtered_sentences = []\n",
        "for sentence in corpus_sentences:\n",
        "  filtered_sentence = remove_stopwords(sentence, spanish_sw)\n",
        "  filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "#print(type(filtered_sentences))\n",
        "#print(len(filtered_sentences))\n",
        "#print(filtered_sentences[0:3])\n",
        "#*** entrenar el modelo Word2Vec con nuestros datos ***\n",
        "# Entrenar el modelo Word2Vec\n",
        "model = Word2Vec(min_count=1,\n",
        "                     window=6,\n",
        "                     vector_size=300,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=4)\n",
        "\n",
        "model.build_vocab(filtered_sentences, progress_per=1000)\n",
        "model.train(filtered_sentences, total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "# Entrenar el modelo Word2Vec (form simplificada)\n",
        "#model = Word2Vec(sentences=filtered_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"\\nCaracteristicas del modelo Word2Vec\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlwSarug9aIf",
        "outputId": "6f965dc8-fe6e-4456-ffb1-559c35f8a897"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Caracteristicas del modelo Word2Vec\n",
            "Word2Vec<vocab=1942, vector_size=300, alpha=0.03>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Preguntar al usuario por una frase\n",
        "user_input = str(input(nombre_usuario + \", introduce su pregunta o algo que te gustaría discutir: \"))\n",
        "\n",
        "\n",
        "# Comparar la frase con la base de datos de frases celebres y buscar la frase celebre con el significado mas cercana a la frase proporcionada por el usuario\n",
        "\n",
        "# Funcion para construir el vector de una frase\n",
        "def get_sentence_vector(words, model):\n",
        "    # determinar el vector de cada palabra en la frase en forma de lista\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    # en caso que vector es vacio generar el vector de ceros y regresarlo\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    # en caso que vector no es vacio generar un promedio y regresarlo\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Procesar la pregunta del usuario para construir su vector\n",
        "user_words = re.sub(r'[^\\w\\s]', '', user_input.lower()).split()\n",
        "filtered_user_input = remove_stopwords(user_words, spanish_sw)\n",
        "user_input_vector = get_sentence_vector(filtered_user_input, model)\n",
        "\n",
        "print(\"Pregunta de \" + nombre_usuario + \": \" + str(user_input))\n",
        "#print(\"Pregunta del ususrio filtrada: \" + str(filtered_user_input))\n",
        "#print(\"Vector del modelo:\")\n",
        "#print(user_input_vector)\n",
        "\n",
        "# Encontrar la frase mas cercana por el signiicado entre frases célebres\n",
        "most_similar_sentence = \"\"\n",
        "max_similarity = -1\n",
        "\n",
        "for i, sentence in enumerate(filtered_sentences):\n",
        "    sentence_vector = get_sentence_vector(sentence, model)\n",
        "    if np.linalg.norm(user_input_vector) != 0 and np.linalg.norm(sentence_vector) != 0:\n",
        "        similarity = cosine_similarity([user_input_vector], [sentence_vector])[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar_sentence = onlysent_tokens[i]\n",
        "\n",
        "\n",
        "if not most_similar_sentence:\n",
        "  print(nombre_usuario + \", lamento, pero no tengo nada que responder en esta ocasión\")\n",
        "else:\n",
        "  print(\"La respuesta es: \" + most_similar_sentence)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YICefY_qA9hC",
        "outputId": "a03eacf5-57a8-46d5-efdf-b718e51c34b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alvaro I, introduce su pregunta o algo que te gustaría discutir: Es importante ser proactivo\n",
            "Pregunta de Alvaro I: Es importante ser proactivo\n",
            "La respuesta es: El ser humano es aquel que no puede ser usado meramente como un medio, sino que debe ser siempre considerado al mismo tiempo como un fin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Despedida\n",
        "print('Hasta la proxima, ' + nombre_usuario + '!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VXMAVCUBi8Q",
        "outputId": "986516d4-a43a-44b0-c110-a32159b6ccb3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasta la proxima, Alvaro I!\n"
          ]
        }
      ]
    }
  ]
}